{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üî¢ NumtaDB Bengali Digit Recognition - Google Colab Training\n",
        "\n",
        "Train a deep learning model to recognize Bengali handwritten digits using Google Colab's free GPU!\n",
        "\n",
        "**Dataset**: [NumtaDB by BengaliAI](https://www.kaggle.com/datasets/BengaliAI/numta) (85,000+ images)\n",
        "\n",
        "**Model**: MobileNetV2 (Fast and accurate)\n",
        "\n",
        "**Expected Results**: ~96-98% accuracy in 30-60 minutes\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Prerequisites\n",
        "\n",
        "Before running this notebook, you need:\n",
        "\n",
        "1. **Kaggle API Credentials** (`kaggle.json`)\n",
        "   - Go to https://www.kaggle.com/settings\n",
        "   - Scroll to \"API\" section\n",
        "   - Click \"Create New API Token\"\n",
        "   - Download `kaggle.json`\n",
        "\n",
        "2. **Trainer Package** (`trainer_package.zip`)\n",
        "   - Run `./prepare_for_colab.sh` on your local machine\n",
        "   - This creates `trainer_package.zip`\n",
        "\n",
        "**You'll be prompted to upload both files when needed!**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Step 1: GPU Check\n",
        "\n",
        "‚ö†Ô∏è **IMPORTANT**: Make sure you've enabled GPU!\n",
        "\n",
        "Go to: **Runtime ‚Üí Change runtime type ‚Üí GPU ‚Üí Save**\n",
        "\n",
        "Then run the cell below to verify:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import sys\n",
        "\n",
        "print(\"Python version:\", sys.version.split()[0])\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No GPU detected! Training will be very slow.\")\n",
        "    print(\"   Go to Runtime ‚Üí Change runtime type ‚Üí GPU ‚Üí Save\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Step 2: Mount Google Drive (Optional but Recommended)\n",
        "\n",
        "Mounting Google Drive allows you to:\n",
        "- Save your trained model permanently\n",
        "- Resume training later\n",
        "- Keep your data between sessions\n",
        "\n",
        "Click the link and authorize access when prompted.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create project directory in Google Drive\n",
        "project_dir = '/content/drive/MyDrive/numtadb-project'\n",
        "os.makedirs(project_dir, exist_ok=True)\n",
        "os.chdir(project_dir)\n",
        "\n",
        "print(f\"\\n‚úÖ Working directory: {os.getcwd()}\")\n",
        "print(\"   Your files will be saved in Google Drive!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Step 3: Install Dependencies\n",
        "\n",
        "Install required Python packages. This takes ~30 seconds.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q kaggle pandas matplotlib seaborn scikit-learn tqdm Pillow\n",
        "\n",
        "print(\"‚úÖ All dependencies installed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Step 4: Setup Kaggle API\n",
        "\n",
        "**Upload your `kaggle.json` file when prompted below!**\n",
        "\n",
        "This file contains your Kaggle API credentials needed to download the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Create Kaggle directory\n",
        "os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "\n",
        "# Check if kaggle.json already exists\n",
        "if os.path.exists('/root/.kaggle/kaggle.json'):\n",
        "    print(\"‚úÖ Kaggle credentials already configured!\")\n",
        "else:\n",
        "    print(\"üì§ Please upload your kaggle.json file:\")\n",
        "    uploaded = files.upload()\n",
        "    \n",
        "    # Move and set permissions\n",
        "    !mv kaggle.json /root/.kaggle/kaggle.json\n",
        "    !chmod 600 /root/.kaggle/kaggle.json\n",
        "    \n",
        "    print(\"\\n‚úÖ Kaggle credentials configured successfully!\")\n",
        "\n",
        "# Verify\n",
        "!kaggle datasets list -s numta | head -3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Step 5: Download NumtaDB Dataset\n",
        "\n",
        "Downloads ~300MB of Bengali handwritten digits from Kaggle.\n",
        "\n",
        "This takes 2-3 minutes depending on connection speed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Check if dataset already exists\n",
        "if os.path.exists('data/raw/training-a.csv'):\n",
        "    print(\"‚úÖ Dataset already downloaded!\")\n",
        "    print(f\"   Location: {os.path.abspath('data/raw')}\")\n",
        "else:\n",
        "    print(\"üì• Downloading NumtaDB dataset from Kaggle...\")\n",
        "    \n",
        "    # Create data directory\n",
        "    os.makedirs('data/raw', exist_ok=True)\n",
        "    os.chdir('data/raw')\n",
        "    \n",
        "    # Download and extract\n",
        "    !kaggle datasets download -d BengaliAI/numta\n",
        "    !unzip -q numta.zip\n",
        "    !rm numta.zip\n",
        "    \n",
        "    # Go back to project root\n",
        "    os.chdir('../..')\n",
        "    \n",
        "    print(\"\\n‚úÖ Dataset downloaded successfully!\")\n",
        "\n",
        "# Show dataset structure\n",
        "print(\"\\nüìä Dataset structure:\")\n",
        "!ls -lh data/raw/*.csv 2>/dev/null || echo \"CSV files loaded\"\n",
        "print(\"\\nImage directories:\")\n",
        "!ls -d data/raw/training-* data/raw/testing-* 2>/dev/null | head -10\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Step 6: Upload Trainer Package\n",
        "\n",
        "**Upload your `trainer_package.zip` file when prompted!**\n",
        "\n",
        "This contains all the training code from your local machine.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Check if trainer folder already exists\n",
        "if os.path.exists('trainer') and os.path.exists('train_model.py'):\n",
        "    print(\"‚úÖ Trainer package already uploaded!\")\n",
        "else:\n",
        "    print(\"üì§ Please upload your trainer_package.zip file:\")\n",
        "    uploaded = files.upload()\n",
        "    \n",
        "    # Extract the zip file\n",
        "    zip_file = list(uploaded.keys())[0]\n",
        "    print(f\"\\nüì¶ Extracting {zip_file}...\")\n",
        "    \n",
        "    with zipfile.ZipFile(zip_file, 'r') as z:\n",
        "        z.extractall('.')\n",
        "    \n",
        "    # Clean up zip file\n",
        "    os.remove(zip_file)\n",
        "    \n",
        "    print(\"‚úÖ Trainer package extracted successfully!\")\n",
        "\n",
        "# Verify files\n",
        "print(\"\\nüìÅ Project files:\")\n",
        "!ls -lh train_model.py 2>/dev/null\n",
        "!ls trainer/ 2>/dev/null | head -10\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Step 7: Configure Training\n",
        "\n",
        "Set up training parameters. Adjust these based on your needs:\n",
        "\n",
        "- **BATCH_SIZE**: Larger = faster training but more memory (reduce if OOM error)\n",
        "- **NUM_EPOCHS**: More epochs = better accuracy but longer training\n",
        "- **LEARNING_RATE**: Adjust if training is unstable\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Add current directory to Python path\n",
        "sys.path.insert(0, os.getcwd())\n",
        "\n",
        "# Import configuration\n",
        "from trainer.config import Config\n",
        "\n",
        "# Configure training parameters\n",
        "Config.BATCH_SIZE = 64          # Reduce to 32 or 16 if you get OOM errors\n",
        "Config.NUM_EPOCHS = 30          # Increase for better accuracy\n",
        "Config.LEARNING_RATE = 0.001\n",
        "Config.NUM_WORKERS = 2          # Colab works best with 2 workers\n",
        "Config.MODEL_NAME = 'mobilenetv2'\n",
        "\n",
        "# Print configuration\n",
        "print(\"\\n‚öôÔ∏è  Training Configuration:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Model:         {Config.MODEL_NAME}\")\n",
        "print(f\"Batch Size:    {Config.BATCH_SIZE}\")\n",
        "print(f\"Epochs:        {Config.NUM_EPOCHS}\")\n",
        "print(f\"Learning Rate: {Config.LEARNING_RATE}\")\n",
        "print(f\"Image Size:    {Config.IMAGE_SIZE}\")\n",
        "print(f\"Device:        {Config.DEVICE}\")\n",
        "print(f\"Workers:       {Config.NUM_WORKERS}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create necessary directories\n",
        "Config.create_dirs()\n",
        "print(\"\\n‚úÖ Configuration complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Step 8: Load Dataset\n",
        "\n",
        "Creates training, validation, and test data loaders.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from trainer.dataset import create_dataloaders\n",
        "import os\n",
        "\n",
        "print(\"üìä Loading dataset...\\n\")\n",
        "\n",
        "# Create data loaders\n",
        "train_loader, val_loader, test_loader = create_dataloaders(\n",
        "    str(Config.DATA_DIR), \n",
        "    Config\n",
        ")\n",
        "\n",
        "# Show dataset statistics\n",
        "print(\"\\n‚úÖ Dataset loaded successfully!\")\n",
        "print(\"\\nüìà Dataset Statistics:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Training samples:   {len(train_loader.dataset):,}\")\n",
        "print(f\"Validation samples: {len(val_loader.dataset):,}\")\n",
        "print(f\"Test samples:       {len(test_loader.dataset):,}\")\n",
        "print(f\"Training batches:   {len(train_loader):,}\")\n",
        "print(f\"Batch size:         {Config.BATCH_SIZE}\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Step 9: Train the Model\n",
        "\n",
        "**This is the main training step!**\n",
        "\n",
        "Training time: ~30-60 minutes depending on:\n",
        "- Dataset size\n",
        "- Number of epochs\n",
        "- GPU type (usually T4 on free tier)\n",
        "\n",
        "You'll see:\n",
        "- Loss and accuracy for each epoch\n",
        "- Progress bars\n",
        "- Validation metrics\n",
        "\n",
        "**The best model will be saved automatically!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from trainer.train import Trainer\n",
        "import time\n",
        "\n",
        "print(\"üöÄ Starting training...\\n\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Create trainer\n",
        "trainer = Trainer(Config)\n",
        "\n",
        "# Train the model\n",
        "metrics = trainer.train(train_loader, val_loader)\n",
        "\n",
        "# Calculate training time\n",
        "training_time = time.time() - start_time\n",
        "hours = int(training_time // 3600)\n",
        "minutes = int((training_time % 3600) // 60)\n",
        "seconds = int(training_time % 60)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéâ Training Complete!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Training time: {hours}h {minutes}m {seconds}s\")\n",
        "print(f\"Best validation accuracy: {max(metrics.get('val_acc', [0])):.2f}%\")\n",
        "print(f\"Model saved at: {Config.CHECKPOINT_DIR}/best_model.pth\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Step 10: Evaluate on Test Set\n",
        "\n",
        "Test the trained model on unseen data to get final accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üìä Evaluating on test set...\\n\")\n",
        "\n",
        "# Evaluate\n",
        "test_loss, test_acc = trainer.validate(test_loader)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìà Final Test Results\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Test Loss:     {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Step 11: Visualize Training Results\n",
        "\n",
        "Generate plots to visualize training progress.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from trainer.visualize import plot_training_history\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "metrics_file = Config.LOG_DIR / 'training_metrics.csv'\n",
        "if os.path.exists(metrics_file):\n",
        "    history = pd.read_csv(metrics_file)\n",
        "    \n",
        "    # Plot\n",
        "    plot_training_history(history, save_path=Config.LOG_DIR / 'training_plots.png')\n",
        "    \n",
        "    print(\"‚úÖ Training plots generated!\")\n",
        "    print(f\"   Saved at: {Config.LOG_DIR}/training_plots.png\")\n",
        "    \n",
        "    # Display in notebook\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No training metrics file found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Step 12: Download Trained Model\n",
        "\n",
        "Download your trained model to use it locally or deploy it!\n",
        "\n",
        "The model file is ~14MB.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "print(\"üì• Preparing files for download...\\n\")\n",
        "\n",
        "# Files to download\n",
        "download_files = [\n",
        "    ('checkpoints/best_model.pth', 'Best model checkpoint'),\n",
        "    ('logs/training_metrics.csv', 'Training history'),\n",
        "    ('logs/training_plots.png', 'Training plots'),\n",
        "]\n",
        "\n",
        "for file_path, description in download_files:\n",
        "    if os.path.exists(file_path):\n",
        "        print(f\"‚úÖ {description}: {file_path}\")\n",
        "        files.download(file_path)\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  {description} not found: {file_path}\")\n",
        "\n",
        "print(\"\\n‚úÖ Download complete!\")\n",
        "print(\"\\nüí° If you used Google Drive, your files are also saved at:\")\n",
        "print(f\"   {os.getcwd()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üéÅ Bonus: Test with Sample Images\n",
        "\n",
        "Try your trained model on sample images from the test set!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Load model\n",
        "model = trainer.model\n",
        "model.eval()\n",
        "\n",
        "# Get some test samples\n",
        "test_iter = iter(test_loader)\n",
        "images, labels = next(test_iter)\n",
        "\n",
        "# Predict\n",
        "with torch.no_grad():\n",
        "    images = images.to(Config.DEVICE)\n",
        "    outputs = model(images)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "# Display first 10 predictions\n",
        "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i in range(10):\n",
        "    img = images[i].cpu().numpy().transpose(1, 2, 0)\n",
        "    img = (img - img.min()) / (img.max() - img.min())  # Normalize for display\n",
        "    \n",
        "    axes[i].imshow(img)\n",
        "    axes[i].axis('off')\n",
        "    \n",
        "    true_label = labels[i].item()\n",
        "    pred_label = predicted[i].item()\n",
        "    \n",
        "    color = 'green' if true_label == pred_label else 'red'\n",
        "    axes[i].set_title(f'True: {true_label}\\nPred: {pred_label}', color=color)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('sample_predictions.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úÖ Sample predictions displayed above!\")\n",
        "print(\"   Green = Correct, Red = Incorrect\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üéØ Next Steps\n",
        "\n",
        "Congratulations! You've successfully trained a Bengali digit recognition model! üéâ\n",
        "\n",
        "### What to do next:\n",
        "\n",
        "1. **Deploy your model**\n",
        "   - Convert to ONNX for web deployment\n",
        "   - Create a web interface\n",
        "   - Build a mobile app\n",
        "\n",
        "2. **Improve accuracy**\n",
        "   - Train for more epochs\n",
        "   - Try different architectures\n",
        "   - Tune hyperparameters\n",
        "   - Add more data augmentation\n",
        "\n",
        "3. **Experiment**\n",
        "   - Try AlexNet model\n",
        "   - Adjust learning rate\n",
        "   - Use different optimizers\n",
        "   - Enable/disable pretrained weights\n",
        "\n",
        "4. **Share your work**\n",
        "   - Create a demo\n",
        "   - Write a blog post\n",
        "   - Share on social media\n",
        "\n",
        "### Resources:\n",
        "\n",
        "- üìÑ [NumtaDB Paper](https://arxiv.org/abs/1806.02452)\n",
        "- üåê [BengaliAI Community](https://www.bengali.ai/)\n",
        "- üìä [Dataset on Kaggle](https://www.kaggle.com/datasets/BengaliAI/numta)\n",
        "- üíª [Project Repository](https://github.com/smafjal/NumtaDB)\n",
        "\n",
        "---\n",
        "\n",
        "**Made with ‚ù§Ô∏è for Bengali language technology**\n",
        "\n",
        "If you found this useful, give it a ‚≠ê on GitHub!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
